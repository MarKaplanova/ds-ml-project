{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from mace.util import (\n",
    "    interpolate_by,\n",
    "    drop_columns,\n",
    "    sort_by\n",
    ")\n",
    "\n",
    "# manipulation \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data prep\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# feature engineering\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# modelling\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# model selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# persistence\n",
    "import joblib\n",
    "#import dill\n",
    "\n",
    "RSEED = 394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of modelling dataset: {df_train.shape}\")\n",
    "print(f\"Shape of prediction] dataset: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which columns differ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cols = [col for col in df_train.columns if col not in df_test.columns]\n",
    "diff_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_cols = [col for col in df_train.columns if col.find('L3') != -1]\n",
    "target_cols = [col for col in df_train.columns if col.find('target') != -1]\n",
    "loc_cols = [col for col in df_train.columns if col.find('Place') != -1 or col.find('Date') != -1 ]\n",
    "weather_cols = [col for col in df_train.columns if col not in poll_cols + target_cols + loc_cols ]\n",
    "\n",
    "print(f\"Number of columns related to:\")\n",
    "print(f\"target: {len(target_cols)}\")\n",
    "print(f\"pollution: {len(poll_cols)}\")\n",
    "print(f\"location: {len(loc_cols)}\")\n",
    "print(f\"weather: {len(weather_cols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many places and dates do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Date.unique()\n",
    "print(f\"There are {df_train.Date.nunique()} dates between {df_train.Date.min()} and {df_train.Date.max()}.\")\n",
    "\n",
    "df_train.Place_ID.unique()\n",
    "print(f\"The number of places is {df_train.Place_ID.nunique()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {df_train.shape[0]} actual observations.\")\n",
    "print(f\"If each place had the full 3 months of observations, we should have a sample size of {94 * 340}.\")\n",
    "\n",
    "print(f\"\\nThis means we do NOT have observations for each place-date pair.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_counts = df_train.groupby('Place_ID').Place_ID.count().unique()\n",
    "print(f\"A place appears between {place_counts.min()} and {place_counts.max()} times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many pollutants do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_pollutants = [NO2, CH4, ....]\n",
    "pollutants = list(set([pol.split('_')[1] for pol in df_train[poll_cols].columns]))\n",
    "pollutants = [pol + \"_AI\" if pol == \"AER\" else pol for pol in pollutants]\n",
    "\n",
    "pollutants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[target_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test\n",
    "\n",
    "Later in this notebook, we perform within-place imputation of various pollutant-related variables. This means we must split dataset into train and test data such that each city appears _either_ in the train data _or_ the test data (but not both). \n",
    "\n",
    "We do this by randomly splitting the list of unique places. Based on this split, we then subset the entire dataset (place-date pairs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.copy()\n",
    "\n",
    "# split dataset into features and target\n",
    "y = df[['target', 'Place_ID']]          # keep place_ID because we need it for the train/test split\n",
    "X = df.drop(target_cols, axis = 1)\n",
    "\n",
    "# obtain unique list of places\n",
    "places = X.Place_ID.unique()\n",
    "\n",
    "# split the cities into train and test\n",
    "places_train, places_test = train_test_split(places, test_size=0.2, random_state=RSEED)\n",
    "\n",
    "# filter the features for train and test cities\n",
    "X_train = X[X['Place_ID'].isin(places_train)]\n",
    "X_test = X[X['Place_ID'].isin(places_test)]\n",
    "\n",
    "# filter the targets for train and test (+ drop the location variable)\n",
    "y_train = y[y['Place_ID'].isin(places_train)].drop('Place_ID', axis = 1)\n",
    "y_test = y[y['Place_ID'].isin(places_test)].drop('Place_ID', axis = 1)\n",
    "\n",
    "# append features and targets for EDA\n",
    "train = pd.concat([X_train, y_train], axis = 1)\n",
    "test = pd.concat([X_test, y_test], axis = 1)\n",
    "\n",
    "# display the shapes of train and test sets\n",
    "print(f\"Number of places in test is {len(places_test)} and in train {len(places_train)}.\")\n",
    "print(\"Train set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Split percentage on set:\", round(X_train.shape[0]/len(X)*100, 4), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns with satellite position information --> not relevant for prediction\n",
    "useless_cols = [col for col in poll_cols if col.find('sensor') != -1 or col.find('solar') != -1]\n",
    "print(\"The pollution features of interest are:\")\n",
    "print(useless_cols)\n",
    "\n",
    "poll_cols = [col for col in poll_cols if col not in useless_cols]\n",
    "\n",
    "X_train = X_train.drop(useless_cols, axis = 1)\n",
    "X_test = X_test.drop(useless_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis & feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[here should be more code for distributions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships among features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tbw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values for location and weather variables. \n",
    "\n",
    "Polution related features have missing values, which are missing in groups on the pollutant level. For instance, if we have 6 variables for methane, values for an observation are either all missing or all non-missing.\n",
    "\n",
    "We assess the missing values, finding that missing values are distributed across the entire 3-month period and across the different cities (i.e. we do _not_ have a situation in which only some cities or some months have observations for a pollutant). \n",
    "\n",
    "Hence, we decide to use linear interpolation for each pollutant on the city level over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess missing values\n",
    "msno.matrix(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendrogram\n",
    "From the dendrogram, we see that the majority of pollution data is either fully or highly coherent.\n",
    "- location and weather data is coherent\n",
    "- CLOUD data is coherent with the exception of L3_CLOUD_cloud_fraction\n",
    "- NO2 data has highly coherent but not fully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dendrogram plot provides a tree-like graph generated through hierarchical clustering and groups together columns \n",
    "# that have strong correlations in nullity.\n",
    "msno.dendrogram(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota bene: Robert Norris - L3_CO_cloud_height was dropped as it has a different dimension to the other features...\n",
    "dendrogram_cluster=['L3_O3_cloud_fraction', \n",
    "                    'L3_CLOUD_cloud_fraction',\n",
    "                    'L3_AER_AI_absorbing_aerosol_index',\n",
    "                    'L3_CLOUD_surface_albedo',\n",
    "                    'L3_NO2_stratospheric_NO2_column_number_density',\n",
    "                    'L3_NO2_cloud_fraction',\n",
    "                    #'L3_CO_cloud_height',\n",
    "                    'L3_SO2_cloud_fraction',\n",
    "                    'L3_SO2_absorbing_aerosol_index',\n",
    "                    'L3_HCHO_tropospheric_HCHO_column_number_density_amf',\n",
    "                    'L3_NO2_tropospheric_NO2_column_number_density',\n",
    "                    'L3_CH4_aerosol_optical_depth'\n",
    "                    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[dendrogram_cluster].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = X_train.sort_values(['Place_ID X Date']).groupby('Place_ID')\n",
    "\n",
    "focus_place = random.choice(places_train)\n",
    "highlighted_cluster = random.choice(dendrogram_cluster)\n",
    "focus_df = groups.get_group(focus_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota bene: Robert Norris - .gca is 'Get the current Axes' which combines all .plot onto same chart...\n",
    "\n",
    "def plotFeatureHighlightingSinglePlace(\n",
    "        groups: pd.core.groupby.generic.DataFrameGroupBy, \n",
    "        highlighted_place: str, \n",
    "        focus_feature: str):\n",
    "    # Plot for a single feature, highlighting a single place\n",
    "    for group in groups:\n",
    "        place=group[0]\n",
    "        df=group[1][[focus_feature] + ['Date']]\n",
    "        df.plot(\n",
    "            kind='line',\n",
    "            x='Date', xticks=[], xlabel='', \n",
    "            figsize=[15,5], ax=plt.gca(),\n",
    "            color='grey', alpha=0.01,\n",
    "            legend=False)\n",
    "        if place == highlighted_place:\n",
    "            df.plot(\n",
    "                kind='line',\n",
    "                x='Date', xticks=[], xlabel='', \n",
    "                figsize=[15,5], ax=plt.gca(),\n",
    "                color='blue',\n",
    "                legend=False)\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single place, highlighting a single feature\n",
    "def plotPlaceHighlightingSingleFeature(\n",
    "    focus_df: pd.DataFrame,\n",
    "    features: list[str], \n",
    "    highlighted_feature: str):\n",
    "    focus_df[dendrogram_cluster + ['Date']].plot(\n",
    "        kind='line',\n",
    "        x='Date', xticks=[], xlabel='', \n",
    "        figsize=[15,5], ax=plt.gca(),\n",
    "        color='grey', alpha=0.2,\n",
    "        legend=False)\n",
    "    # It is easier to plot all in grey and then choose a single one to highlight than mess with the 'color cycler'...\n",
    "    focus_df[[highlighted_cluster] + ['Date']].plot(\n",
    "        kind='line',\n",
    "        x='Date', xticks=[], xlabel='', \n",
    "        figsize=[15,5], ax=plt.gca(),\n",
    "        color='blue',\n",
    "        legend=False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for a specific place and feature\n",
    "sample_feature = 'L3_SO2_absorbing_aerosol_index'\n",
    "sample_place = places_train[6]\n",
    "sample_place_df = groups.get_group(sample_place)\n",
    "\n",
    "plotFeatureHighlightingSinglePlace(groups, sample_place, sample_feature)  \n",
    "plotPlaceHighlightingSingleFeature(sample_place_df, dendrogram_cluster, sample_feature)          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!MK finished cleaning code here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_missing = X.groupby('Place_ID').count().sort_values('L3_CH4_aerosol_height')\n",
    "\n",
    "counts_missing.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers are applied completely separately in parallel\n",
    "# any columns not listed for any of the transformers will be dropped when remainder='drop' (default)\n",
    "# cannot 'simply' pickle lambda functions\n",
    "\n",
    "lmp = Pipeline([\n",
    "    ('clean', FunctionTransformer(\n",
    "        drop_columns, \n",
    "        feature_names_out='one-to-one', \n",
    "        validate=False, \n",
    "        kw_args={'columns': useless_cols})),\n",
    "    ('sort', FunctionTransformer(\n",
    "        sort_by, \n",
    "        feature_names_out='one-to-one', \n",
    "        validate=False, \n",
    "        kw_args={'columns': ['Place_ID X Date']})),\n",
    "    ('linear', FunctionTransformer(\n",
    "        interpolate_by, \n",
    "        validate=False,\n",
    "        kw_args={'columns': ['Place_ID']})),\n",
    "    ('drop', ColumnTransformer([('drop', 'drop', loc_cols)], remainder='passthrough')),\n",
    "    ('mean', SimpleImputer()),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('regressor', LinearRegression()) \n",
    "])\n",
    "\n",
    "y_ = df_train[['target']]\n",
    "X_ = df_train.drop(target_cols, axis = 1)\n",
    "\n",
    "lmp.fit(X_, y_)\n",
    "joblib.dump(lmp, 'models/ols.joblib')\n",
    "#dill.dump(lmp, open(\"models/ols.dill\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation for missing values\n",
    "- we decided to use linear interpolation on the level of a place. \n",
    "- This means we need to: \n",
    "    - order data by place and date\n",
    "    - interpolate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear imputation of missing values\n",
    "X_train.sort_values(['Place_ID X Date'])\n",
    "X_test.sort_values(['Place_ID X Date'])\n",
    "\n",
    "# Stepwise imputation: \n",
    "# 1. linear interpolation by place (back and forward)\n",
    "\n",
    "X_train_impute = X_train.groupby('Place_ID').apply(lambda group: group.interpolate(method='linear', limit_direction = 'both')).reset_index(drop = 1)\n",
    "X_test_impute = X_test.groupby('Place_ID').apply(lambda group: group.interpolate(method='linear', limit_direction = 'both')).reset_index(drop = 1)\n",
    "\n",
    "# 2. mean fill if no better option using simple imputer\n",
    "impute_mean = SimpleImputer()\n",
    "impute_mean.fit(X_train_impute[poll_cols])\n",
    "\n",
    "X_train_impute[poll_cols] = impute_mean.transform(X_train_impute[poll_cols])\n",
    "X_test_impute[poll_cols] = impute_mean.transform(X_test_impute[poll_cols])\n",
    "\n",
    "# check on the one place for which we needed the simple imputer\n",
    "# X_test_impute[X_test_impute.Place_ID == '5IUK9TG']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota bene: Robert Norris - groupby seems to perform at least a partial copy so\n",
    "# we must regroup after imputation...\n",
    "X_train_impute_groups=X_train_impute.groupby('Place_ID')\n",
    "place_df = X_train_impute_groups.get_group(sample_place)\n",
    "\n",
    "plotFeatureHighlightingSinglePlace(X_train_impute_groups, sample_place, sample_feature)  \n",
    "plotPlaceHighlightingSingleFeature(place_df, dendrogram_cluster, sample_feature)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there are no missing values\n",
    "assert X_train_impute.isnull().sum().sum() == 0\n",
    "assert X_test_impute.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace dataset with imputed dataset\n",
    "X_train = X_train_impute.copy()\n",
    "X_test = X_test_impute.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------\n",
    "# feature sets \n",
    "\n",
    "all_features = weather_cols + poll_cols \n",
    "\n",
    "#---------------------------------------------------\n",
    "# pipelines\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# linear model\n",
    "lm = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('regressor', LinearRegression()) \n",
    "])\n",
    "\n",
    "# polynomial model\n",
    "pm = Pipeline([\n",
    "    ('scaler', scaler), \n",
    "    ('tranformer', PolynomialFeatures()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# knn model\n",
    "knn = Pipeline([\n",
    "    ('scaler', scaler), # why this one? we can think about it more, could use standard scaler?\n",
    "    ('regressor', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "# decision tree\n",
    "dt = Pipeline([\n",
    "    ('regressor', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "# random forest\n",
    "rf = None\n",
    "\n",
    "# xgboost\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# function definitions\n",
    "\n",
    "def evaluate_model(model, features, Xtest = X_test, Xtrain = X_train): \n",
    "    y_test_pred = model.predict(Xtest[features])\n",
    "    y_train_pred = model.predict(Xtrain[features])\n",
    "    \n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test  = r2_score(y_test, y_test_pred)\n",
    "    rmse_train = mean_squared_error(y_train, y_train_pred, squared = False)\n",
    "    rmse_test  = mean_squared_error(y_test, y_test_pred, squared = False)\n",
    "\n",
    "    print(f\"R2 score on train: {round(r2_train, 3)}\")\n",
    "    print(f\"R2 score on test: {round(r2_test, 3)}\")\n",
    "    print(\"---\" * 10)\n",
    "    print(f\"RMSE on train: {round(rmse_train, 3)}\")\n",
    "    print(f\"RMSE on test: {round(rmse_test, 3)}\")\n",
    "\n",
    "    return y_test_pred, y_train_pred, rmse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "- hypothesis: there is less PM2.5 in the atmosphere when it is windy\n",
    "- method: OLS\n",
    "\n",
    "- model\n",
    "$$ y = b_0 + b_1 * x_1 + \\epsilon $$\n",
    "\n",
    "- estimated model\n",
    "$$ \\hat{PM}_{2.5} = \\hat{b}_0 + \\hat{b}_1 * wind $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = ['relative_humidity_2m_above_ground']\n",
    "\n",
    "lm.fit(X_train[base_features], y_train)\n",
    "\n",
    "y_pred_base, _, rmse_base = evaluate_model(lm, base_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot y against X\n",
    "plt.scatter(X_train[base_features], y_train)\n",
    "plt.xlabel('weather')\n",
    "plt.ylabel('pm2.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred_base)\n",
    "plt.xlabel('true PM2.5')\n",
    "plt.ylabel('predicted PM2.5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS with all features (scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_train[all_features], y_train)\n",
    "\n",
    "y_pred_lm, _, rmse_lm = evaluate_model(lm, all_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN regression (scaled)\n",
    "- tried initially unscaled version with truly *horrible* results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train[all_features], y_train)\n",
    "\n",
    "y_pred_knn, _, rmse_knn = evaluate_model(knn, all_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm.fit(X_train[all_features], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple decision tree (scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train[all_features], y_train)\n",
    "\n",
    "y_pred_dt, _, rmse_dt = evaluate_model(dt, all_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DT Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitters = ['best', 'random']\n",
    "criterion = ['squared_error']\n",
    "depths = [2,4,_final]\n",
    "splits = [2,4,_final]\n",
    "leaves = [4,_final,16]\n",
    "param_grid = {#\"regressor__splitter\": splitters,\n",
    "              \"regressor__criterion\" : criterion,\n",
    "              \"regressor__max_depth\" : depths,\n",
    "              \"regressor__min_samples_split\": splits,\n",
    "              \"regressor__min_samples_leaf\": leaves\n",
    "             }\n",
    "dt_gs = GridSearchCV(dt, param_grid,\n",
    "                  cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "dt_gs.fit(X_train[all_features], y_train)\n",
    "end = time()\n",
    "print(f'fit in {end - start}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score:', round(dt_gs.best_score_, 3))\n",
    "print('Best parameters:', dt_gs.best_params_)\n",
    "\n",
    "y_pred_dt_gs, _, rmse_dt_gs = evaluate_model(dt_gs, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1 = XGBRegressor(\n",
    "    booster='gbtree',\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0._final,\n",
    "    colsample_bytree=0._final,\n",
    "    objective= 'reg:squarederror',\n",
    "    random_state=RSEED)\n",
    "\n",
    "xgb = Pipeline([\n",
    "    #('scaler', scaler),\n",
    "    ('regressor', gb1)\n",
    "])\n",
    "\n",
    "xgb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = range(3,10,2)\n",
    "weights = range(1,6,2)\n",
    "param_grid = {\"regressor__max_depth\": depths,\n",
    "              \"regressor__min_child_weight\": weights\n",
    "             }\n",
    "\n",
    "xgb_gs = GridSearchCV(xgb, param_grid, \n",
    "                  cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "xgb_gs.fit(X_train[all_features], y_train)\n",
    "print(f'fit in {time() - start}s')\n",
    "y_pred_xgb, _, rmse_xgb = evaluate_model(xgb_gs, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs.best_params_, xgb_gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'reg:squarederror',\n",
    "    seed=RSEED)\n",
    "\n",
    "xgb.fit(X_train[all_features], y_train)\n",
    "y__, _, rmse__ = evaluate_model(xgb, all_features)\n",
    "\n",
    "xgb.feature_importances_\n",
    "feature_imp = pd.DataFrame(\n",
    "    {'features': all_features,\n",
    "     'importance': xgb.feature_importances_,\n",
    "    }\n",
    ")\n",
    "\n",
    "feature_imp = feature_imp.sort_values('importance',ascending=False).head(20)\n",
    "best_features = list(feature_imp.features)\n",
    "\n",
    "# plot best features\n",
    "plt.barh(feature_imp.features, feature_imp.importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unimp_cols = [col for col in X_train if col not in best_features and col not in loc_cols]\n",
    "unimp_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers are applied completely separately in parallel\n",
    "# any columns not listed for any of the transformers will be dropped when remainder='drop' (default)\n",
    "# cannot 'simply' pickle lambda functions\n",
    "\n",
    "xgb_opt = XGBRegressor(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=7,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.2,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha = 100, \n",
    "    objective= 'reg:squarederror',\n",
    "    seed=RSEED)\n",
    "\n",
    "gs_final = Pipeline([\n",
    "    ('clean', FunctionTransformer(\n",
    "        drop_columns, \n",
    "        feature_names_out='one-to-one', \n",
    "        validate=False, \n",
    "        kw_args={'columns': useless_cols + unimp_cols})),\n",
    "    ('sort', FunctionTransformer(\n",
    "        sort_by, \n",
    "        feature_names_out='one-to-one', \n",
    "        validate=False, \n",
    "        kw_args={'columns': ['Place_ID X Date']})),\n",
    "    ('linear', FunctionTransformer(\n",
    "        interpolate_by, \n",
    "        validate=False,\n",
    "        kw_args={'columns': ['Place_ID']})),\n",
    "    ('drop', ColumnTransformer([('drop', 'drop', loc_cols)], remainder='passthrough')),\n",
    "    ('mean', SimpleImputer()),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('regressor', xgb_opt) \n",
    "])\n",
    "\n",
    "## load data\n",
    "df_train_est = pd.read_csv('data/train.csv')\n",
    "\n",
    "X_train = df_train_est[df_train_est['Place_ID'].isin(places_train)]\n",
    "X_test = df_train_est[df_train_est['Place_ID'].isin(places_test)]\n",
    "\n",
    "y_train = X_train[['target']]\n",
    "X_train = X_train.drop(target_cols, axis = 1)\n",
    "\n",
    "y_test = X_test[['target']]\n",
    "X_test = X_test.drop(target_cols, axis = 1)\n",
    "\n",
    "gs_final.fit(X_train, y_train)\n",
    "joblib.dump(gs_final, 'models/gs_final.joblib')\n",
    "\n",
    "_, _, _ = evaluate_model(\n",
    "    gs_final, \n",
    "    weather_cols + poll_cols + useless_cols + loc_cols,\n",
    "    X_test, X_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
