{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 1,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "# manipulation \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data prep\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# feature engineering\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# modelling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "RSEED = 394"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Place_ID X Date</th>\n",
       "      <th>Date</th>\n",
       "      <th>Place_ID</th>\n",
       "      <th>target</th>\n",
       "      <th>target_min</th>\n",
       "      <th>target_max</th>\n",
       "      <th>target_variance</th>\n",
       "      <th>target_count</th>\n",
       "      <th>precipitable_water_entire_atmosphere</th>\n",
       "      <th>...</th>\n",
       "      <th>L3_SO2_sensor_zenith_angle</th>\n",
       "      <th>L3_SO2_solar_azimuth_angle</th>\n",
       "      <th>L3_SO2_solar_zenith_angle</th>\n",
       "      <th>L3_CH4_CH4_column_volume_mixing_ratio_dry_air</th>\n",
       "      <th>L3_CH4_aerosol_height</th>\n",
       "      <th>L3_CH4_aerosol_optical_depth</th>\n",
       "      <th>L3_CH4_sensor_azimuth_angle</th>\n",
       "      <th>L3_CH4_sensor_zenith_angle</th>\n",
       "      <th>L3_CH4_solar_azimuth_angle</th>\n",
       "      <th>L3_CH4_solar_zenith_angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>010Q650 X 2020-01-02</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>010Q650</td>\n",
       "      <td>38.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>769.50</td>\n",
       "      <td>92</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>38.593017</td>\n",
       "      <td>-61.752587</td>\n",
       "      <td>22.363665</td>\n",
       "      <td>1793.793579</td>\n",
       "      <td>3227.855469</td>\n",
       "      <td>0.010579</td>\n",
       "      <td>74.481049</td>\n",
       "      <td>37.501499</td>\n",
       "      <td>-62.142639</td>\n",
       "      <td>22.545118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>010Q650 X 2020-01-03</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>010Q650</td>\n",
       "      <td>39.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1319.85</td>\n",
       "      <td>91</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>59.624912</td>\n",
       "      <td>-67.693509</td>\n",
       "      <td>28.614804</td>\n",
       "      <td>1789.960449</td>\n",
       "      <td>3384.226562</td>\n",
       "      <td>0.015104</td>\n",
       "      <td>75.630043</td>\n",
       "      <td>55.657486</td>\n",
       "      <td>-53.868134</td>\n",
       "      <td>19.293652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>010Q650 X 2020-01-04</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>010Q650</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1181.96</td>\n",
       "      <td>96</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>49.839714</td>\n",
       "      <td>-78.342701</td>\n",
       "      <td>34.296977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>010Q650 X 2020-01-05</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>010Q650</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1113.67</td>\n",
       "      <td>96</td>\n",
       "      <td>6.911948</td>\n",
       "      <td>...</td>\n",
       "      <td>29.181258</td>\n",
       "      <td>-73.896588</td>\n",
       "      <td>30.545446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>010Q650 X 2020-01-06</td>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>010Q650</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1164.82</td>\n",
       "      <td>95</td>\n",
       "      <td>13.900001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797294</td>\n",
       "      <td>-68.612480</td>\n",
       "      <td>26.899694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       Place_ID X Date        Date Place_ID  target  target_min   \n",
       "0           0  010Q650 X 2020-01-02  2020-01-02  010Q650    38.0        23.0  \\\n",
       "1           1  010Q650 X 2020-01-03  2020-01-03  010Q650    39.0        25.0   \n",
       "2           2  010Q650 X 2020-01-04  2020-01-04  010Q650    24.0         8.0   \n",
       "3           3  010Q650 X 2020-01-05  2020-01-05  010Q650    49.0        10.0   \n",
       "4           4  010Q650 X 2020-01-06  2020-01-06  010Q650    21.0         9.0   \n",
       "\n",
       "   target_max  target_variance  target_count   \n",
       "0        53.0           769.50            92  \\\n",
       "1        63.0          1319.85            91   \n",
       "2        56.0          1181.96            96   \n",
       "3        55.0          1113.67            96   \n",
       "4        52.0          1164.82            95   \n",
       "\n",
       "   precipitable_water_entire_atmosphere  ...  L3_SO2_sensor_zenith_angle   \n",
       "0                             11.000000  ...                   38.593017  \\\n",
       "1                             14.600000  ...                   59.624912   \n",
       "2                             16.400000  ...                   49.839714   \n",
       "3                              6.911948  ...                   29.181258   \n",
       "4                             13.900001  ...                    0.797294   \n",
       "\n",
       "   L3_SO2_solar_azimuth_angle  L3_SO2_solar_zenith_angle   \n",
       "0                  -61.752587                  22.363665  \\\n",
       "1                  -67.693509                  28.614804   \n",
       "2                  -78.342701                  34.296977   \n",
       "3                  -73.896588                  30.545446   \n",
       "4                  -68.612480                  26.899694   \n",
       "\n",
       "   L3_CH4_CH4_column_volume_mixing_ratio_dry_air  L3_CH4_aerosol_height   \n",
       "0                                    1793.793579            3227.855469  \\\n",
       "1                                    1789.960449            3384.226562   \n",
       "2                                            NaN                    NaN   \n",
       "3                                            NaN                    NaN   \n",
       "4                                            NaN                    NaN   \n",
       "\n",
       "   L3_CH4_aerosol_optical_depth  L3_CH4_sensor_azimuth_angle   \n",
       "0                      0.010579                    74.481049  \\\n",
       "1                      0.015104                    75.630043   \n",
       "2                           NaN                          NaN   \n",
       "3                           NaN                          NaN   \n",
       "4                           NaN                          NaN   \n",
       "\n",
       "   L3_CH4_sensor_zenith_angle  L3_CH4_solar_azimuth_angle   \n",
       "0                   37.501499                  -62.142639  \\\n",
       "1                   55.657486                  -53.868134   \n",
       "2                         NaN                         NaN   \n",
       "3                         NaN                         NaN   \n",
       "4                         NaN                         NaN   \n",
       "\n",
       "   L3_CH4_solar_zenith_angle  \n",
       "0                  22.545118  \n",
       "1                  19.293652  \n",
       "2                        NaN  \n",
       "3                        NaN  \n",
       "4                        NaN  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "# load data\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of modelling dataset: {df_train.shape}\")\n",
    "print(f\"Shape of prediction] dataset: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which columns differ? "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target', 'target_min', 'target_max', 'target_variance', 'target_count']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "diff_cols = [col for col in df_train.columns if col not in df_test.columns]\n",
    "diff_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data features "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 5,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_cols = [col for col in df_train.columns if col.find('L3') != -1]\n",
    "target_cols = [col for col in df_train.columns if col.find('target') != -1]\n",
    "loc_cols = [col for col in df_train.columns if col.find('Place') != -1 or col.find('Date') != -1 ]\n",
    "weather_cols = [col for col in df_train.columns if col not in poll_cols + target_cols + loc_cols ]\n",
    "\n",
    "print(f\"Number of columns related to:\")\n",
    "print(f\"target: {len(target_cols)}\")\n",
    "print(f\"pollution: {len(poll_cols)}\")\n",
    "print(f\"location: {len(loc_cols)}\")\n",
    "print(f\"weather: {len(weather_cols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many places and dates do we have? "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 6,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Date.unique()\n",
    "print(f\"There are {df_train.Date.nunique()} dates between {df_train.Date.min()} and {df_train.Date.max()}.\")\n",
    "\n",
    "df_train.Place_ID.unique()\n",
    "print(f\"The number of places is {df_train.Place_ID.nunique()}.\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 7,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {df_train.shape[0]} actual observations.\")\n",
    "print(f\"If each place had the full 3 months of observations, we should have a sample size of {94 * 340}.\")\n",
    "\n",
    "print(f\"\\nThis means we do NOT have observations for each place-date pair.\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 8,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "place_counts = df_train.groupby('Place_ID').Place_ID.count().unique()\n",
    "print(f\"A place appears between {place_counts.min()} and {place_counts.max()} times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many pollutants do we have? "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 9,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_pollutants = [NO2, CH4, ....]\n",
    "pollutants = list(set([pol.split('_')[1] for pol in df_train[poll_cols].columns]))\n",
    "pollutants = [pol + \"_AI\" if pol == \"AER\" else pol for pol in pollutants]\n",
    "\n",
    "pollutants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< Updated upstream
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>target_min</th>\n",
       "      <th>target_max</th>\n",
       "      <th>target_variance</th>\n",
       "      <th>target_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>769.50</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1319.85</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1181.96</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1113.67</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1164.82</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  target_min  target_max  target_variance  target_count\n",
       "0    38.0        23.0        53.0           769.50            92\n",
       "1    39.0        25.0        63.0          1319.85            91\n",
       "2    24.0         8.0        56.0          1181.96            96\n",
       "3    49.0        10.0        55.0          1113.67            96\n",
       "4    21.0         9.0        52.0          1164.82            95"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "df_train[target_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test\n",
    "\n",
    "Later in this notebook, we perform within-place imputation of various pollutant-related variables. This means we must split dataset into train and test data such that each city appears _either_ in the train data _or_ the test data (but not both). \n",
    "\n",
    "We do this by randomly splitting the list of unique places. Based on this split, we then subset the entire dataset (place-date pairs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.copy()\n",
    "\n",
    "# split dataset into features and target\n",
    "y = df[['target', 'Place_ID']]          # keep place_ID because we need it for the train/test split\n",
    "X = df.drop(target_cols, axis = 1)\n",
    "\n",
    "# obtain unique list of places\n",
    "places = X.Place_ID.unique()\n",
    "\n",
    "# split the cities into train and test\n",
    "places_train, places_test = train_test_split(places, test_size=0.2, random_state=RSEED)\n",
    "\n",
    "# filter the features for train and test cities\n",
    "X_train = X[X['Place_ID'].isin(places_train)]\n",
    "X_test = X[X['Place_ID'].isin(places_test)]\n",
    "\n",
    "# filter the targets for train and test (+ drop the location variable)\n",
    "y_train = y[y['Place_ID'].isin(places_train)].drop('Place_ID', axis = 1)\n",
    "y_test = y[y['Place_ID'].isin(places_test)].drop('Place_ID', axis = 1)\n",
    "\n",
    "# append features and targets for EDA\n",
    "train = pd.concat([X_train, y_train], axis = 1)\n",
    "test = pd.concat([X_test, y_test], axis = 1)\n",
    "\n",
    "# display the shapes of train and test sets\n",
    "print(f\"Number of places in test is {len(places_test)} and in train {len(places_train)}.\")\n",
    "print(\"Train set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Split percentage on set:\", round(X_train.shape[0]/len(X)*100, 4), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns with satellite position information --> not relevant for prediction\n",
    "useless_cols = [col for col in poll_cols if col.find('sensor') != -1 or col.find('solar') != -1]\n",
    "print(\"The pollution features of interest are:\")\n",
    "print(useless_cols)\n",
    "\n",
    "poll_cols = [col for col in poll_cols if col not in useless_cols]\n",
    "\n",
    "X_train = X_train.drop(useless_cols, axis = 1)\n",
    "X_test = X_test.drop(useless_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis & feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[here should be more code for distributions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships among features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tbw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values for location and weather variables. \n",
    "\n",
    "Polution related features have missing values, which are missing in groups on the pollutant level. For instance, if we have 6 variables for methane, values for an observation are either all missing or all non-missing.\n",
    "\n",
    "We assess the missing values, finding that missing values are distributed across the entire 3-month period and across the different cities (i.e. we do _not_ have a situation in which only some cities or some months have observations for a pollutant). \n",
    "\n",
    "Hence, we decide to use linear interpolation for each pollutant on the city level over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess missing values\n",
    "msno.matrix(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendrogram\n",
    "From the dendrogram, we see that the majority of pollution data is either fully or highly coherent.\n",
    "- location and weather data is coherent\n",
    "- CLOUD data is coherent with the exception of L3_CLOUD_cloud_fraction\n",
    "- NO2 data has highly coherent but not fully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dendrogram plot provides a tree-like graph generated through hierarchical clustering and groups together columns \n",
    "# that have strong correlations in nullity.\n",
    "msno.dendrogram(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota bene: Robert Norris - L3_CO_cloud_height was dropped as it has a different dimension to the other features...\n",
    "dendrogram_cluster=['L3_O3_cloud_fraction', \n",
    "                    'L3_CLOUD_cloud_fraction',\n",
    "                    'L3_AER_AI_absorbing_aerosol_index',\n",
    "                    'L3_CLOUD_surface_albedo',\n",
    "                    'L3_NO2_stratospheric_NO2_column_number_density',\n",
    "                    'L3_NO2_cloud_fraction',\n",
    "                    #'L3_CO_cloud_height',\n",
    "                    'L3_SO2_cloud_fraction',\n",
    "                    'L3_SO2_absorbing_aerosol_index',\n",
    "                    'L3_HCHO_tropospheric_HCHO_column_number_density_amf',\n",
    "                    'L3_NO2_tropospheric_NO2_column_number_density',\n",
    "                    'L3_CH4_aerosol_optical_depth'\n",
    "                    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[dendrogram_cluster].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = X_train.sort_values(['Place_ID X Date']).groupby('Place_ID')\n",
    "\n",
    "focus_place = random.choice(places_train)\n",
    "highlighted_cluster = random.choice(dendrogram_cluster)\n",
    "focus_df = groups.get_group(focus_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota bene: Robert Norris - .gca is 'Get the current Axes' which combines all .plot onto same chart...\n",
    "\n",
    "def plotFeatureHighlightingSinglePlace(\n",
    "        groups: pd.core.groupby.generic.DataFrameGroupBy, \n",
    "        highlighted_place: str, \n",
    "        focus_feature: str):\n",
    "    # Plot for a single feature, highlighting a single place\n",
    "    for group in groups:\n",
    "        place=group[0]\n",
    "        df=group[1][[focus_feature] + ['Date']]\n",
    "        df.plot(\n",
    "            kind='line',\n",
    "            x='Date', xticks=[], xlabel='', \n",
    "            figsize=[15,5], ax=plt.gca(),\n",
    "            color='grey', alpha=0.01,\n",
    "            legend=False)\n",
    "        if place == highlighted_place:\n",
    "            df.plot(\n",
    "                kind='line',\n",
    "                x='Date', xticks=[], xlabel='', \n",
    "                figsize=[15,5], ax=plt.gca(),\n",
    "                color='blue',\n",
    "                legend=False)\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single place, highlighting a single feature\n",
    "def plotPlaceHighlightingSingleFeature(\n",
    "    focus_df: pd.DataFrame,\n",
    "    features: list[str], \n",
    "    highlighted_feature: str):\n",
    "    focus_df[dendrogram_cluster + ['Date']].plot(\n",
    "        kind='line',\n",
    "        x='Date', xticks=[], xlabel='', \n",
    "        figsize=[15,5], ax=plt.gca(),\n",
    "        color='grey', alpha=0.2,\n",
    "        legend=False)\n",
    "    # It is easier to plot all in grey and then choose a single one to highlight than mess with the 'color cycler'...\n",
    "    focus_df[[highlighted_cluster] + ['Date']].plot(\n",
    "        kind='line',\n",
    "        x='Date', xticks=[], xlabel='', \n",
    "        figsize=[15,5], ax=plt.gca(),\n",
    "        color='blue',\n",
    "        legend=False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for a specific place and feature\n",
    "sample_feature = 'L3_SO2_absorbing_aerosol_index'\n",
    "sample_place = places_train[6]\n",
    "sample_place_df = groups.get_group(sample_place)\n",
    "\n",
    "plotFeatureHighlightingSinglePlace(groups, sample_place, sample_feature)  \n",
    "plotPlaceHighlightingSingleFeature(sample_place_df, dendrogram_cluster, sample_feature)          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!MK finished cleaning code here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_missing = X.groupby('Place_ID').count().sort_values('L3_CH4_aerosol_height')\n",
    "\n",
    "counts_missing.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation for missing values\n",
    "- we decided to use linear interpolation on the level of a place. \n",
    "- This means we need to: \n",
    "    - order data by place and date\n",
    "    - interpolate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear imputation of missing values\n",
    "X_train.sort_values(['Place_ID X Date'])\n",
    "X_test.sort_values(['Place_ID X Date'])\n",
    "\n",
    "# Stepwise imputation: \n",
    "# 1. linear interpolation by place (back and forward)\n",
    "\n",
    "X_train_impute = X_train.groupby('Place_ID').apply(lambda group: group.interpolate(method='linear', limit_direction = 'both')).reset_index(drop = 1)\n",
    "X_test_impute = X_test.groupby('Place_ID').apply(lambda group: group.interpolate(method='linear', limit_direction = 'both')).reset_index(drop = 1)\n",
    "\n",
    "# 2. mean fill if no better option using simple imputer\n",
    "impute_mean = SimpleImputer()\n",
    "impute_mean.fit(X_train_impute[poll_cols])\n",
    "\n",
    "X_train_impute[poll_cols] = impute_mean.transform(X_train_impute[poll_cols])\n",
    "X_test_impute[poll_cols] = impute_mean.transform(X_test_impute[poll_cols])\n",
    "\n",
    "# check on the one place for which we needed the simple imputer\n",
    "# X_test_impute[X_test_impute.Place_ID == '5IUK9TG']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota bene: Robert Norris - groupby seems to perform at least a partial copy so\n",
    "# we must regroup after imputation...\n",
    "X_train_impute_groups=X_train_impute.groupby('Place_ID')\n",
    "place_df = X_train_impute_groups.get_group(sample_place)\n",
    "\n",
    "plotFeatureHighlightingSinglePlace(X_train_impute_groups, sample_place, sample_feature)  \n",
    "plotPlaceHighlightingSingleFeature(place_df, dendrogram_cluster, sample_feature)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there are no missing values\n",
    "assert X_train_impute.isnull().sum().sum() == 0\n",
    "assert X_test_impute.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace dataset with imputed dataset\n",
    "X_train = X_train_impute.copy()\n",
    "X_test = X_test_impute.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< Updated upstream
=======
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------\n",
    "# feature sets \n",
    "\n",
    "all_features = weather_cols + poll_cols \n",
    "\n",
    "#---------------------------------------------------\n",
    "# pipelines\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# linear model\n",
    "lm = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('regressor', LinearRegression()) \n",
    "])\n",
    "\n",
    "# polynomial model\n",
    "pm = Pipeline([\n",
    "    ('scaler', scaler), \n",
    "    ('tranformer', PolynomialFeatures()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# knn model\n",
    "knn = Pipeline([\n",
    "    ('scaler', scaler), # why this one? we can think about it more, could use standard scaler?\n",
    "    ('regressor', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "# decision tree\n",
    "dt = Pipeline([\n",
    "    ('regressor', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "# random forest\n",
    "rf = None\n",
    "\n",
    "# xgboost\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# function definitions\n",
    "\n",
    "def evaluate_model(model, features): \n",
    "    y_test_pred = model.predict(X_test[features])\n",
    "    y_train_pred = model.predict(X_train[features])\n",
    "    \n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test  = r2_score(y_test, y_test_pred)\n",
    "    rmse_train = mean_squared_error(y_train, y_train_pred, squared = False)\n",
    "    rmse_test  = mean_squared_error(y_test, y_test_pred, squared = False)\n",
    "\n",
    "    print(f\"R2 score on train: {round(r2_train, 3)}\")\n",
    "    print(f\"R2 score on test: {round(r2_test, 3)}\")\n",
    "    print(\"---\" * 10)\n",
    "    print(f\"RMSE on train: {round(rmse_train, 3)}\")\n",
    "    print(f\"RMSE on test: {round(rmse_test, 3)}\")\n",
    "\n",
    "    return y_test_pred, y_train_pred, rmse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> Stashed changes
    "## Baseline model\n",
    "\n",
    "- hypothesis: there is less PM2.5 in the atmosphere when it is windy\n",
    "- method: OLS\n",
    "\n",
    "- model\n",
    "$$ y = b_0 + b_1 * x_1 + \\epsilon $$\n",
    "\n",
    "- estimated model\n",
    "$$ PM_{2.5} = \\hat{b}_0 + \\hat{b}_1 * wind $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "\n",
    "# select features\n",
    "base_features = ['relative_humidity_2m_above_ground']\n",
    "\n",
    "# fit linear regression\n",
    "base = LinearRegression()\n",
    "base.fit(X_train[base_features], y_train)\n",
    "\n",
    "# predict values for test data\n",
    "y_pred_base = base.predict(X_test[base_features])\n",
    "e_base = y_test - y_pred_base\n",
    "\n",
    "# predict values for train data (for checks)\n",
    "y_pred_base_train = base.predict(X_train[base_features])\n",
    "\n",
    "# calculate rmse of the model\n",
    "rmse = mean_squared_error(y_test, y_pred_base, squared = False)\n",
    "rmse\n",
    "\n",
    "r2 = r2_score(y_test, y_pred_base)\n",
    "print(r2)\n",
    "\n",
    "# concat true and predicted values\n",
    "base_plot = pd.DataFrame(np.c_[y_test, y_pred_base, e_base])\n",
    "base_plot\n",
    "\n",
    "# print model coefficients\n",
    "print(f\" The model is: \\n PM2.5 = {base.coef_} + {base.intercept_[0]} * humidity\")\n",
    "\n",
    "# r2_score(y_train, y_pred_train)\n",
    "base.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[base_features], y_train)\n",
    "plt.xlabel('weather')\n",
    "plt.ylabel('pm2.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< Updated upstream
    "## Advanced models"
=======
    "## OLS with all features (scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_train[all_features], y_train)\n",
    "\n",
    "y_pred_lm, _, rmse_lm = evaluate_model(lm, all_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN regression (scaled)\n",
    "- tried initially unscaled version with truly *horrible* results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train[all_features], y_train)\n",
    "\n",
    "y_pred_knn, _, rmse_knn = evaluate_model(knn, all_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial model"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ols, knn, dt, rf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## next: \n",
    "# 1. deal with corner values forward backward\n",
    "# 2. deal with that one city which has 0 observations for CH4\n",
    "# 3. plot results and celebrate\n",
    "\n",
<<<<<<< Updated upstream
    "# # Plot imputed data\n",
    "# linear_int['MaxSpeed'][:100].plot(color='red', marker='o', linestyle='dotted')\n",
    "# df['MaxSpeed'][:100].plot(title='MaxSpeed', marker='o')\n",
    "\n",
    "# splitting tasks: \n",
    "# - finish interpolation \n",
    "# - understand pollution variables & summary stastistics & correlations\n",
    "# - scaling + feature engineering \n",
    "# - plan which model to use & implement \n",
    "# [- presentation]\n"
=======
    "y_pred_dt, _, rmse_dt = evaluate_model(dt, all_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DT Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitters = ['best', 'random']\n",
    "criterion = ['squared_error']\n",
    "depths = [2,4,8]\n",
    "splits = [2,4,8]\n",
    "leaves = [4,8,16]\n",
    "param_grid = {#\"regressor__splitter\": splitters,\n",
    "              \"regressor__criterion\" : criterion,\n",
    "              \"regressor__max_depth\" : depths,\n",
    "              \"regressor__min_samples_split\": splits,\n",
    "              \"regressor__min_samples_leaf\": leaves\n",
    "             }\n",
    "dt_gs = GridSearchCV(dt, param_grid,\n",
    "                  cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "dt_gs.fit(X_train[all_features], y_train)\n",
    "end = time()\n",
    "print(f'fit in {end - start}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score:', round(dt_gs.best_score_, 3))\n",
    "print('Best parameters:', dt_gs.best_params_)\n",
    "\n",
    "y_pred_dt_gs, _, rmse_dt_gs = evaluate_model(dt_gs, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1 = XGBRegressor(\n",
    "    booster='gbtree',\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'reg:squarederror',\n",
    "    random_state=RSEED)\n",
    "\n",
    "xgb = Pipeline([\n",
    "    #('scaler', scaler),\n",
    "    ('regressor', gb1)\n",
    "])\n",
    "\n",
    "xgb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = range(3,10,2)\n",
    "weights = range(1,6,2)\n",
    "param_grid = {\"regressor__max_depth\": depths,\n",
    "              \"regressor__min_child_weight\": weights\n",
    "             }\n",
    "\n",
    "xgb_gs = GridSearchCV(xgb, param_grid, \n",
    "                  cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "xgb_gs.fit(X_train[all_features], y_train)\n",
    "print(f'fit in {time() - start}s')\n",
    "y_pred_xgb, _, rmse_xgb = evaluate_model(xgb_gs, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs.best_params_, xgb_gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# goals\n",
    "\n",
    "next task: \n",
    " - run polynomial (with grid search)\n",
    "\n",
    "how to improve a model: \n",
    " - improve model: \n",
    "    - feature selection: \n",
    "        - look into feature importance\n",
    "    - Ridge/Lasso\n",
    "        - can be done within XGB\n",
    "    - hyperparameter tuning\n",
    "        - could do Gridsearch/Randomsearch\n",
    "\n",
    "\n",
    " - interpret model \n",
    " - can we predict quality in cities around the world? --> yes, but what are the limitations"
>>>>>>> Stashed changes
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
